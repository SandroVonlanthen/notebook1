{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Computing: Notebook 1\n",
    "# Online data collection using the requests library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please inlcude your names below and edit the name of the file to include the names of the people answering\n",
    "\n",
    "# Students: Max Kiefer , Sandro Vonlanthen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As step 0, pick your favorite Wikipedia page, open it in the browser, and then save it as an html file. Now open it in the browser as well as in a text editor and look at the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the requests library you can retrieve the html source of the page, in a response object (using requests.get(“url”)). The response object you received has content that you can access calling the .text function on it.\n",
    "\n",
    "Call text and save the result in a file, then open the file in a browser and check whether you successfully saved the page. Note, you will only be able to open the file in the browser if you give it an html extension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Basic web crawling (10 points)\n",
    "\n",
    "URLs have specific formats, for example any Wikipedia page will be of the format https://en.wikipedia.org/wiki/Pythonidae where the last word is the topic of the article.\n",
    "Next, we want to automate this saving process using the requests library and making automated requests to Wikipedia.\n",
    "\n",
    "Exercise: Pick 5 different words, and write code that loops through these words, and retrieves the html content for each associated wikipedia page, and saves the html text as wiki_htmls/[word].html files. (Choose words that actually have associated wiki pages). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\notebook1\\SC_Notebook1_NAME1_NAME2.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebook1/SC_Notebook1_NAME1_NAME2.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/notebook1/SC_Notebook1_NAME1_NAME2.ipynb#ch0000006?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhtml\u001b[39;00m \u001b[39mimport\u001b[39;00m fromstring\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebook1/SC_Notebook1_NAME1_NAME2.ipynb#ch0000006?line=3'>4</a>\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Mathematics\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/notebook1/SC_Notebook1_NAME1_NAME2.ipynb#ch0000006?line=5'>6</a>\u001b[0m res \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from lxml.html import fromstring\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Mathematics\"\n",
    "\n",
    "res = requests.get(url)\n",
    "source = fromstring(res.content)\n",
    "paragraph = '\\n'.join([item.text_content() for item in source.xpath('//p[following::h2[2][span=\"History\"]]')])\n",
    "print(paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) URL formats (10 points)\n",
    "\n",
    "What is the common URL in the case of Google searches? And in the case of Yelp? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The basic Google search is https://www.google.com/search?q= and for Yelp it is the Yelp Listing URL that appears in the address bar. For example, www.yelp.com/biz/unanderra-mower-centre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what happens to the URL if you want to define the location as well as the type of venue you are looking for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In the case of an URL, the venue is a place, especially the one where a given event is to happen while location is a particular point or place in physical space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you find more search parameters for either of the two pages that you can define via the URLs? What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "They are comprised of a key and a value pair, separated by an equal sign. Multiple parameters can be added to a single page by using an ampersand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) HTML content basics (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "res = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the BeautifulSoup parser library we will parse the webpage that you just saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import BeautifulSoup, our parser library\n",
    "# And make a soup object out of the html of the page\n",
    "\n",
    "# in case bs4 throws error try\n",
    "# !pip install --upgrade html5lib==1.0b8\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a nice version using prettify\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we can find all instances of a tag at once: Try to predict what the following command will return: `soup.find_all('p')` and then call it to check if you were right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We guess that it will find all the html elements in p and output them. Yes as the information gives the html elements of the text and the parts of the p class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the second element of this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<p class=\"inner-text\">\n",
    "                 Second paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the text inside the second element of the list, using the .text on the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inner-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you try to find a specific element on a page you can reach it by finding classes or IDs of the elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p', class_='outer-text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How many elements would it return for 'inner-text'? Guess, and check your guess by using the find_all command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First paragraph and second paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4) Finding elements in the browser (50 points)\n",
    "Since every web page is different and html can get very large and messy, the easiest way to find elements that you are interested in is to start from the browser window. So next we will quickly look at how to find elements using the developer tools in your browser. Open the following webpage in your browser (preferably Chrome): http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579#.Wkwh8VQ-fVo \n",
    "\n",
    "Find the developer tools in your browser. (In Chrome, it's view --> developer --> developer tools or Control+Shift+C on Windows and Command+Shift+C on Mac) You should end up with a panel at the bottom or the right side of the browser like what you see below. Make sure the Elements panel is highlighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579\")\n",
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to find a specific element, you can right click on it on the page and select \"inspect\". This will also open up the developer tools window. For example if we want to extract the current temperature value:\n",
    "\n",
    "<img src=\"inspect.png\">\n",
    "\n",
    "<img src=\"inspect_class.png\">\n",
    "\n",
    "<br><br>\n",
    "1. (5 points) Using the find function, extract and print out the current temperature from the page.\n",
    "2. (5 points) Do the same with the value in Celsius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fill out and print a full sentence describing the temperature in F and C. \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"http://forecast.weather.gov/MapClick.php?lat=21.3049&lon=-157.8579\")\n",
    "soup = BeautifulSoup(res.content, 'html.parser')\n",
    "\n",
    "\n",
    "temp_F = soup.find(class_ =  \"myforecast-current-lrg\").getText()\n",
    "temp_C  = soup.find(class_ =  \"myforecast-current-sm\").getText()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. (20 points) In this exercise we will extract each half day's forecast from the extended forecast on the weather report page. <br>\n",
    "    a. Find the container for the extended forecast on the weather page we just downloaded. <br>\n",
    "    b. Make a list with all forecast items (overnight, Wednesday, Wednesday night, etc) <br>\n",
    "    c. For each time period, print out the name of the period, the short description of the expected weather conditions, and the temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher  :  ['Aspen School District', 'The Stony Brook School', 'Anchorage School District', 'Institute Of Reading Development Inc', 'US Justice, Bureau of Prisons/Federal Prison...', 'School Administrative Unit 70', 'Oregon Episcopal School', 'Richmond Public Schools', 'Maple Run Unified School District', 'Nauset Public Schools', 'Chesapeake Lighthouse Foundation', \"Prince George's County Public Schools\", 'North Kingstown School Department', 'Aspen School District', 'John Thomas Dye School']\n",
      "lawyer  :  ['Senate, NYS', 'Adobe', 'Major League Baseball', 'Red Bull', 'AvantStay', 'PACCAR', 'Gilead Sciences', 'State of Rhode Island', 'States United Democracy Center', 'Best Best & Krieger LLP', 'Corporate Tools', 'US DHS Headquarters', 'Red Bull', 'US Executive Office for U.S. Attorneys and the...', 'Onni Group']\n",
      "data-scientist  :  ['Nike', 'John Deere', 'Cigna', 'Microsoft', 'US Customs and Border Protection', 'Amazon Web Services, Inc.', 'Madison Square Garden Entertainment', 'Convoy', 'Vizient, Inc.', 'Spotify', 'Conde Nast', 'IBM', 'Cerebral', 'US Internal Revenue Service', 'Scripps Health']\n"
     ]
    }
   ],
   "source": [
    "# For each time period print out something like: \n",
    "# Overnight the weather will be mostly clear and breezy and the temperature will be 65F. \n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urlbase = \" https://www.indeed.com/q-\"\n",
    "listjobs= ['teacher','lawyer','data-scientist']\n",
    "urlfinisch = \"-jobs.html\"\n",
    "returnDict= {}\n",
    "\n",
    "for elements in listjobs:\n",
    "    \n",
    "    url = urlbase+elements+urlfinisch\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.content, 'html.parser')\n",
    "    companyList =[]\n",
    "    firstcompany= soup.find_all(class_ =\"companyName\")\n",
    "    for company in firstcompany:\n",
    "        companyList. append(company.getText())\n",
    "    returnDict[elements]= companyList\n",
    " \n",
    "\n",
    "for key, value in returnDict.items():\n",
    "    print(key, ' : ', value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. (20 points) Take a list of jobs (e.g.['teacher', 'lawyer', 'data-scientist']). For each job save the html of the result of searching it on indeed. The url of a result page looks like: https://www.indeed.com/q-data-scientist-jobs.html. \n",
    "<br>\n",
    "For each job find the names of the companies from the first result page.  Make a dictionary where the keys are the jobs and value is a list of the company names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher  :  ['North Kingstown School Department', 'Centennial School District', 'North Kingstown School Department', 'Aspen School District', 'Institute Of Reading Development Inc', 'Anchorage School District', 'Richmond Public Schools', 'US Bureau of Indian Education', 'The Stony Brook School', 'Edmonds School District', 'US Justice, Bureau of Prisons/Federal Prison...', 'Chesapeake Lighthouse Foundation', 'Winchester Public Schools', 'Paulding County Schools', 'Dallas Independent School District']\n",
      "lawyer  :  ['Contract Counselors', 'U.S. Department of Education Office of General...', 'Axiom Law', 'Best Best & Krieger LLP', 'Animal Defense Legal Fund', 'PSI Services LLC', 'Axiom Law', 'Atencio, Hall and McKernan, PLLC', 'Consilio Services', 'Longevity InTime', 'Axiom Law', 'Kowert, Hood, Munyon, Rankin & Goetzel', 'Red Bull', 'Robins Kaplan LLP', 'Lengea Law']\n",
      "data-scientist  :  ['Quadrant Resource', 'TechData Service Company, LLC', 'WorkCog', 'Nike', 'Reed Technologies Inc', 'John Deere', 'CDC Foundation ( Contract)', 'Radcube LLC', 'Cigna', 'Microsoft', 'BlueSky Technology Solutions', 'Lark Health', 'Data Ninjas Inc', 'US Customs and Border Protection', 'Amazon Web Services, Inc.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "#\n",
    "urlbase = \" https://www.indeed.com/q-\"\n",
    "listjobs= ['teacher','lawyer','data-scientist']\n",
    "urlfinisch = \"-jobs.html\"\n",
    "returnDict= {}\n",
    "\n",
    "for elemnts in listjobs:\n",
    "    \n",
    "    url = urlbase+elemnts+urlfinisch\n",
    "    res = request.urlopen(url)\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    companyList =[]\n",
    "    firstcompany= soup.find_all(class_ =\"companyName\")\n",
    "    for company in firstcompany:\n",
    "        companyList. append(company.getText())\n",
    "    returnDict[elemnts]= companyList\n",
    " \n",
    "\n",
    "for key, value in returnDict.items():\n",
    "    print(key, ' : ', value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Headers (25 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every request you send has a so called HTTP header (unrelated to the content of the message), for example to communicate the size of the message, the browser from which the request is coming from, or what kind of response it is expecting back in the response. \n",
    "\n",
    "1) Read up on this: What parts does a request contain exactly and what is the purpose of a header? \n",
    "    \n",
    "    A header is used to pass information between the client and the server. There are two separat hedears one with your request and one with the respond. \n",
    "\n",
    "2) Look in the browser: Take a URL and find the request header using the developer tools in your browser. (Hint: you will need to look inside 'network'). \n",
    "\n",
    "3) If you don’t tell python otherwise, it will use a default header when sending requests. What is this default when you use the requests library?\n",
    "\n",
    "\n",
    "4) The requests library allows to specify the headers of your request exactly. Set the header of your request (for the  URL you previously picked) to be the one copied from your browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your chosen URL:\n",
    "\n",
    "    https://stackoverflow.com/questions/52570933/what-is-the-request-header-by-default-in-python-requests\n",
    "\n",
    "Default header of Python requests: \n",
    "    User-Agent: \n",
    "    \n",
    "    python-requests/2.27.1 \n",
    "    \n",
    "    Accept-Encoding: gzip, deflate\n",
    "    \n",
    "    Accept: */* \n",
    "    \n",
    "    Connection': keep-alive\n",
    "\n",
    "Header in your browser: \n",
    "\n",
    "    GET /questions/52570933/what-is-the-request-header-by-default-in-python-requests HTTP/2\n",
    "\n",
    "    Host: stackoverflow.com\n",
    "\n",
    "    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\n",
    "\n",
    "    Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\n",
    "\n",
    "    Accept-Language: en-US,en;q=0.5\n",
    "\n",
    "    Accept-Encoding: gzip, deflate, br\n",
    "\n",
    "    Referer: https://www.google.com/\n",
    "\n",
    "    Connection: keep-alive\n",
    "\n",
    "    Cookie: prov=027c9553-0c4f-5a19-43a6-81f2aa78c4f1; _ga=GA1.2.9612263.1601718222; __gads=ID=076b06a0bc6ee9cf-2270a5dc60cd003a:T=1616424877:S=ALNI_MYxVIkTMVzrUCWQbQo_jz4Ig-Qr2Q; OptanonConsent=isIABGlobal=false&datestamp=Tue+Apr+06+2021+11%3A34%3A30+GMT%2B0200+(Central+European+Summer+Time)&version=6.10.0&hosts=&landingPath=NotLandingPage&groups=C0003%3A1%2CC0004%3A1%2CC0002%3A1%2CC0001%3A1; OptanonAlertBoxClosed=2021-04-06T09:34:30.144Z; __gpi=UID=00000221a204aabf:T=1645990110:RT=1645990110:S=ALNI_MZw-f4WqT6TTBDgA5JMrf6mkm1Srg; _gid=GA1.2.648197543.1647509312\n",
    "\n",
    "    Upgrade-Insecure-Requests: 1\n",
    "\n",
    "    Sec-Fetch-Dest: document\n",
    "\n",
    "    Sec-Fetch-Mode: navigate\n",
    "\n",
    "    Sec-Fetch-Site: cross-site\n",
    "\n",
    "    Cache-Control: max-age=0\n",
    "\n",
    "    TE: trailers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Now compare the response headers for the same URL in the browser, and by calling a function on the response object in your code. What differences do you see? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response header in your browser: \n",
    "\n",
    "    HTTP/2 200 OK\n",
    "\n",
    "    cache-control: private\n",
    "\n",
    "    content-type: text/html\n",
    "\n",
    "    charset=utf-8\n",
    "\n",
    "    content-encoding: gzip\n",
    "\n",
    "    strict-transport-security: max-age=15552000\n",
    "\n",
    "    x-frame-options: SAMEORIGIN\n",
    "\n",
    "    x-request-guid: 1655a82d-b1ab-434b-bb8e-797e7e73e7bd\n",
    "\n",
    "    feature-policy: microphone 'none'; speaker 'none'\n",
    "\n",
    "    content-security-policy: upgrade-insecure-requests; frame-ancestors 'self' https://stackexchange.com\n",
    "\n",
    "    accept-ranges: bytes\n",
    "\n",
    "    date: Fri, 18 Mar 2022 10:28:50 GMT\n",
    "\n",
    "    via: 1.1 varnish\n",
    "\n",
    "    x-served-by: cache-fra19167-FRA\n",
    "\n",
    "    x-cache: MISS\n",
    "\n",
    "    x-cache-hits: 0\n",
    "\n",
    "    x-timer: S1647599331.754125,VS0,VE103\n",
    "\n",
    "    vary: Accept-Encoding,Fastly-SSL\n",
    "  \n",
    "    x-dns-prefetch-control: off\n",
    "  \n",
    "    X-Firefox-Spdy: h2\n",
    "\n",
    "Response header in the response in python: \n",
    "\n",
    "    Connection : keep-alive \n",
    "\n",
    "    cache-control: private\n",
    "\n",
    "    content-type: text/html\n",
    "\n",
    "    charset=utf-8 \n",
    "\n",
    "    content-encoding: gzi\n",
    "\n",
    "    strict-transport-security: max-age=15552000\n",
    "\n",
    "    x-frame-options: SAMEORIGIN\n",
    "\n",
    "    x-request-guid: 5d4bbc98-bead-417b-ab09-a8f6d99d80c6\n",
    "\n",
    "    feature-policy: \"microphone none; speaker none\"\n",
    "\n",
    "    content-security-policy: upgrade-insecure-requests; frame-ancestors 'self' https://stackexchange.com\"\n",
    "\n",
    "    Accept-Ranges: 'bytes\n",
    "\n",
    "    Date: Fri, 18 Mar 2022 12:52:53 GMT \n",
    "\n",
    "    Via: 1.1 varnish\n",
    "\n",
    "    X-Served-By: cache-fra19130-FRA \n",
    "\n",
    "    X-Cache: MISS\n",
    "\n",
    "    X-Cache-Hits: 0\n",
    "\n",
    "    X-Timer: S1647607973.015960,VS0,VE103\n",
    "\n",
    "    Vary: Accept-Encoding,Fastly-SSL \n",
    "\n",
    "    X-DNS-Prefetch-Control: off \n",
    "\n",
    "    transfer-encoding: chunzed\n",
    "\n",
    "Difference: The respons is quite identical expect for the time and for the status respons."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
